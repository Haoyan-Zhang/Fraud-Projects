{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "pd.set_option('display.max_columns', None)\n",
    "import calendar\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "import logging\n",
    "import time\n",
    "import scipy.stats as sps\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('card transactions.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Merchnum na\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "\n",
    "for i in data[data['Merchnum'].isnull()].index:\n",
    "    description = data.loc[i,'Merch description']\n",
    "    index = data[data['Merch description']==description].index\n",
    "    for j in index:\n",
    "        if isNaN(data.loc[j,'Merchnum']) or j == i:\n",
    "            continue\n",
    "        else:\n",
    "            data.loc[i,'Merchnum'] = data.loc[j,'Merchnum']\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_data=data.sort_values(by=['Merch description'])\n",
    "order_data = order_data.reset_index()\n",
    "\n",
    "for i in range(len(order_data)):\n",
    "    if isNaN(order_data.loc[i,'Merchnum']):\n",
    "        order_data.loc[i,'Merchnum'] = str(order_data.loc[i-1,'Merchnum'])[:]+order_data.loc[i,'Merch description'][-2:]\n",
    "        for j in range(i+1,len(order_data)):\n",
    "            if order_data.loc[j,'Merch description'] == order_data.loc[i,'Merch description']:\n",
    "                order_data.loc[j,'Merchnum'] = order_data.loc[i,'Merchnum']\n",
    "            else:\n",
    "                break\n",
    "data=order_data.sort_values(by=['index']).reset_index().drop(columns=['level_0','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in the null zip code with state code using some zip code in that state\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'AK'),'Merch zip'] = 99500\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'AL'),'Merch zip'] = 35000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'AZ'),'Merch zip'] = 85000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'CA'),'Merch zip'] = 90000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'CO'),'Merch zip'] = 80000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'CT'),'Merch zip'] = 6000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'DC'),'Merch zip'] = 20000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'FL'),'Merch zip'] = 32000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'GA'),'Merch zip'] = 30000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'IA'),'Merch zip'] = 50000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'ID'),'Merch zip'] = 83200\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'IL'),'Merch zip'] = 60000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'IN'),'Merch zip'] = 46000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'KS'),'Merch zip'] = 66000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'KY'),'Merch zip'] = 40000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'LA'),'Merch zip'] = 70000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'MA'),'Merch zip'] = 1000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'MD'),'Merch zip'] = 20600\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'MI'),'Merch zip'] = 48000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'MN'),'Merch zip'] = 55000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'MO'),'Merch zip'] = 63000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'MS'),'Merch zip'] = 38600\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'MT'),'Merch zip'] = 59000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'NC'),'Merch zip'] = 27000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'ND'),'Merch zip'] = 58000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'NE'),'Merch zip'] = 68000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'NH'),'Merch zip'] = 3000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'NJ'),'Merch zip'] = 7000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'NM'),'Merch zip'] = 87000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'NV'),'Merch zip'] = 88900\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'NY'),'Merch zip'] = 500\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'OH'),'Merch zip'] = 43000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'OK'),'Merch zip'] = 73000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'OR'),'Merch zip'] = 97000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'PA'),'Merch zip'] = 15000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'RI'),'Merch zip'] = 2800\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'SD'),'Merch zip'] = 57000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'TX'),'Merch zip'] = 73300\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'UT'),'Merch zip'] = 84000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'VA'),'Merch zip'] = 20100\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'WA'),'Merch zip'] = 98000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'WI'),'Merch zip'] = 53000\n",
    "data.loc[isNaN(data['Merch zip']) & (data['Merch state'] == 'WV'),'Merch zip'] = 82000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['Merch zip'] == 926) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 929) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 934) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 902) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 738) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 90805) & (data['Merch state'].isna()),'Merch state'] = 'CA'\n",
    "data.loc[(data['Merch zip'] == 95461) & (data['Merch state'].isna()),'Merch state'] = 'CA'\n",
    "data.loc[(data['Merch zip'] == 914) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 680) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 681) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 623) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 726) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 936) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 12108) & (data['Merch state'].isna()),'Merch state'] = 'NY'\n",
    "data.loc[(data['Merch zip'] == 791) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 738) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 907) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 922) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 920) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 801) & (data['Merch state'].isna()),'Merch state'] = 'PR'\n",
    "data.loc[(data['Merch zip'] == 31040) & (data['Merch state'].isna()),'Merch state'] = 'GA'\n",
    "data.loc[(data['Merch zip'] == 38117) & (data['Merch state'].isna()),'Merch state'] = 'TN'\n",
    "data.loc[(data['Merch zip'] == 41160) & (data['Merch state'].isna()),'Merch state'] = 'KY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing value for state\n",
    "for i in data[data['Merch state'].isnull()].index:\n",
    "    if isNaN(data.loc[i,'Merch zip']):\n",
    "        continue\n",
    "    else:     \n",
    "        zipnum = data.loc[i,'Merch zip']\n",
    "        index = data[data['Merch zip']==zipnum].index\n",
    "        for j in index:\n",
    "            if isNaN(data.loc[j,'Merch state']) or j == i:\n",
    "                continue\n",
    "            else:\n",
    "                data.loc[i,'Merch state'] = data.loc[j,'Merch state']\n",
    "                break\n",
    "    \n",
    "data['Merch zip'].fillna('Unknown',inplace=True)\n",
    "data['Merch state'].fillna('Unknown',inplace=True)\n",
    "data.drop(52714, axis=0, inplace=True)\n",
    "data = data[data['Transtype']=='P']\n",
    "data.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "date_index = pd.to_datetime(pd.Series(data['Date'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['first digit'] = (data['Amount']*100).astype(str).str.slice(0,1,1)\n",
    "data['Cardnum Benford'] = np.nan\n",
    "data['Merchnum Benford'] = np.nan\n",
    "data['isFEDEX'] = data['Merch description'].str.slice(0,5,1)\n",
    "\n",
    "for i in data.index:\n",
    "    for j in ['Cardnum','Merchnum']:\n",
    "        digit_before = data[(data['Recnum'] <= i+1) & (data[j] == data.loc[i,j])& (data['isFEDEX'] != 'FEDEX')]\n",
    "        digit_count = pd.DataFrame(digit_before.groupby('first digit')['Recnum'].count())\n",
    "        n_low = digit_count.loc[(digit_count.index == '1')|(digit_count.index == '2'),'Recnum'].sum()\n",
    "        n_high = digit_count['Recnum'].sum() - n_low\n",
    "        if n_high == 0:\n",
    "            n_high = 1\n",
    "        R = 1.096*n_low/n_high\n",
    "        if R != 0:\n",
    "            U = max(R,1/R)\n",
    "        else:\n",
    "            U = R\n",
    "        data.loc[i,j + ' Benford'] = U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dow']=data.Date.apply(lambda x:calendar.day_name[x.weekday()])\n",
    "#encode day-of-week risk table \n",
    "train_test=data[data.Date<'2010-11-01']\n",
    "c=4;nmid=20;\n",
    "y_avg=train_test['Fraud'].mean()\n",
    "y_dow=train_test.groupby('dow')['Fraud'].mean()\n",
    "num=train_test.groupby('dow').size()\n",
    "y_dow_smooth=y_avg+(y_dow-y_avg)/(1+np.exp(-(num-nmid)/c))\n",
    "data['dow_risk']=data.dow.map(y_dow_smooth)\n",
    "\n",
    "yd=y_dow.reset_index()\n",
    "yd=yd.iloc[[1,5,6,4,0,2,3]].reset_index()[['dow','Fraud']]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='white')\n",
    "# Defining the plot size \n",
    "plt.figure(figsize=(15, 8)) \n",
    "\n",
    "# Defining the values for x-axis, y-axis \n",
    "# and from which datafarme the values are to be picked \n",
    "plots = sns.barplot(x='dow',y='Fraud',data=yd, color='#a4866f')\n",
    "  \n",
    "# Iterrating over the bars one-by-one \n",
    "for bar in plots.patches: \n",
    "    \n",
    "  # Using Matplotlib's annotate function and \n",
    "  # passing the coordinates where the annotation shall be done \n",
    "  # x-coordinate: bar.get_x() + bar.get_width() / 2 \n",
    "  # y-coordinate: bar.get_height() \n",
    "  # free space to be left to make graph pleasing: (0, 8) \n",
    "  # ha and va stand for the horizontal and vertical alignment \n",
    "    plots.annotate(format(bar.get_height(), '.5f'),  \n",
    "                   (bar.get_x() + bar.get_width() / 2,  \n",
    "                    bar.get_height()), ha='center', va='center', \n",
    "                   size=15, xytext=(0, 8), \n",
    "                   textcoords='offset points') \n",
    "\n",
    "\n",
    "# add tick positions and labels\n",
    "\n",
    "#tickpositions_y=[0.013, 0.0135, 0.014,0.0145,0.015,0.0155,0.016]\n",
    "#ticklables_y=[0.0130, 0.0135, 0.0140,0.0145,0.0150,0.0155,0.0160]\n",
    "\n",
    "# Change the default x-axis and y-axis tick labels and positions\n",
    "#plt.yticks(tickpositions_y,ticklables_y)\n",
    "  \n",
    "# Setting the label for y-axis \n",
    "plt.ylabel(\"\") \n",
    "plt.xlabel(\"\") \n",
    "\n",
    "  \n",
    "# Setting the title for the graph \n",
    "plt.title(\"Fraud Proportion of Each Day\", size=20, pad=20, loc='left') \n",
    "#plt.ylim(0.013, 0.016)\n",
    "\n",
    "y_avg=df['Fraud'].mean()\n",
    "plt.axhline(y=y_avg,color='#d7c18a',linestyle='--')\n",
    "plt.annotate('---',(4.7, 0.015),color='#d7c18a', size=30)\n",
    "plt.annotate('Average Fraud Proportion',(5.0, 0.015),color='black', size=15)\n",
    "plt.xticks(fontsize=14,rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "# Fianlly showing the plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create entities\n",
    "data['Cardnum_Merchnum'] = data['Cardnum'].astype(str) + data['Merchnum']\n",
    "data['num_description'] = data['Merchnum'] + data['Merch description']\n",
    "data['num_state'] = data['Merchnum'] + data['Merch state']\n",
    "data['num_zip'] = data['Merchnum'] + data['Merch zip'].astype(str)\n",
    "data['description_state'] = data['Merch description'] + data['Merch state']\n",
    "data['description_zip'] = data['Merch description'] + data['Merch zip'].astype(str)\n",
    "data['state_zip'] = data['Merch state'] + data['Merch zip'].astype(str)\n",
    "data['num_description_state'] = data['num_description'] + data['Merch state']\n",
    "data['num_description_zip'] = data['num_description'] + data['Merch zip'].astype(str)\n",
    "data['description_state_zip'] = data['description_state'] + data['Merch zip'].astype(str)\n",
    "data['num_description_state_zip'] = data['num_description'] + data['state_zip']\n",
    "\n",
    "\n",
    "data['Cardnum_Merchnum_zip'] = data['Cardnum'].astype(str)  + data['Merchnum']+ data['Merch zip'].astype(str)\n",
    "data['Cardnum_Merchnum_Des_zip'] = data['Cardnum_Merchnum_zip']+ data['Merch description']\n",
    "\n",
    "\n",
    "data['Cardnum_zip'] = data['Cardnum'].astype(str) + data['Merch zip'].astype(str)\n",
    "data['Cardnum_state'] = data['Cardnum'].astype(str) + data['Merch state']\n",
    "data['Cardnum_Merch_Des'] = data['Cardnum'].astype(str) + data['Merch description']\n",
    "\n",
    "entities = ['Cardnum', 'Merchnum', 'Cardnum_Merchnum', 'Merch zip', 'Merch state',\n",
    "           'num_description', 'num_state', 'num_zip', 'description_state', 'description_zip',\n",
    "           'state_zip', 'num_description_state', 'num_description_zip', \n",
    "            'description_state_zip', 'num_description_state_zip','Cardnum_Merchnum_zip', \n",
    "            'Cardnum_Merchnum_Des_zip','Cardnum_zip','Cardnum_state','Cardnum_Merch_Des']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save copy\n",
    "df1 = data.copy()\n",
    "final = data.copy()\n",
    "df1['check_date']=df1.Date\n",
    "df1['check_record']=df1.Recnum\n",
    "\n",
    "#Create day since, velocity variables\n",
    "for entity in entities:\n",
    "    df_1=df1[['Recnum','Date',entity]]\n",
    "    df_r=df1[['check_record','check_date',entity]]\n",
    "    temp=pd.merge(df_1,df_r,left_on=entity, right_on=entity)\n",
    "    \n",
    "    #day since\n",
    "    day_since_df=temp[temp.Recnum>temp.check_record][['Recnum','Date','check_date']] \\\n",
    "        .groupby('Recnum')[['Date','check_date']].last() # last ? days\n",
    "    mapper=(day_since_df.Date - day_since_df.check_date).dt.days\n",
    "    final[entity + '_day_since']=final.Recnum.map(mapper)\n",
    "    final[entity + '_day_since'].fillna(365,inplace=True)\n",
    "    print(f'\\n{entity}_day_since ----> Done')\n",
    "    \n",
    "    #velocity\n",
    "    for offset_t in [0,1,3,7,14,30]:\n",
    "        count_day_df=temp[(temp.check_date >= (temp.Date - dt.timedelta(offset_t))) & (temp.Recnum >= temp.check_record)]\n",
    "        col_name=f'{entity}_count_{offset_t}'\n",
    "        mapper2=count_day_df.groupby('Recnum')[entity].count()\n",
    "        final[col_name]=final.Recnum.map(mapper2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Amount Variables, frequency variables\n",
    "#Create statistics over __ days\n",
    "summary_df = pd.DataFrame()\n",
    "for date in date_index:\n",
    "    #print('start', date)\n",
    "    temp_save = data[data['Date'] == date][['Recnum']]\n",
    "    #print(temp_save)\n",
    "    for entity in entities:\n",
    "        #print('start', entity)\n",
    "        for offset_t in [0,1,3,7,14,30]:\n",
    "            select_df = data[(data.Date >= (date - dt.timedelta(offset_t))) & (data.Date <= date)]\n",
    "            summary = select_df[[entity, 'Amount']].groupby(entity).agg({'Amount': ['mean','max','median','sum','count']})['Amount']\n",
    "            \n",
    "            #print(summary)\n",
    "            merge_data = pd.merge(data.loc[data.Date==date,['Recnum',entity]],summary,left_on=entity, right_on=entity)[['Recnum','mean','max','median','sum','count']]\n",
    "            merge_data = merge_data.rename(columns={\"mean\": entity+\"_mean_\"+\"Amount_\"+str(offset_t),\n",
    "                                                    \"max\": entity+\"_max_\"+\"Amount_\"+str(offset_t),\n",
    "                                                   \"median\": entity+\"_median_\"+\"Amount_\"+str(offset_t),\n",
    "                                                   \"sum\": entity+\"_sum_\"+\"Amount_\"+str(offset_t),\n",
    "                                                   \"count\": entity+\"_freq_\"+str(offset_t)\n",
    "                                                   })\n",
    "            #print(merge_data)\n",
    "            temp_save = pd.merge(temp_save,merge_data,left_on='Recnum', right_on='Recnum')\n",
    "    summary_df = pd.concat([summary_df,temp_save])\n",
    "    \n",
    "print('Finish Statistics over __ days')\n",
    "\n",
    "#Merge the new variable to original df\n",
    "final = pd.merge(final,summary_df,left_on='Recnum', right_on='Recnum')\n",
    "\n",
    "#Calculate the Actual division\n",
    "for entity in entities:\n",
    "    for offset_t in [0,1,3,7,14,30]:\n",
    "        name_dict = {\"mean\": entity+\"_mean_\"+\"Amount_\"+str(offset_t),\n",
    "                                                    \"max\": entity+\"_max_\"+\"Amount_\"+str(offset_t),\n",
    "                                                   \"median\": entity+\"_median_\"+\"Amount_\"+str(offset_t),\n",
    "                                                   \"sum\": entity+\"_sum_\"+\"Amount_\"+str(offset_t),\n",
    "                                                   \"count\": entity+\"_freq_\"+str(offset_t)\n",
    "                    }\n",
    "        for stats in name_dict:\n",
    "            final['Actual/'+name_dict[stats]] = final['Amount']/final[name_dict[stats]]\n",
    "            \n",
    "print('Finish Actual / Statistics over __ days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number Relative velocity\n",
    "for att in entities:\n",
    "    for d in ['0','1']:\n",
    "        for dd in ['7','14','30']:\n",
    "            final[att + '_count_' + d + '_by_' +dd] \\\n",
    "            =final[att + '_count_' +d]/(final[att + '_count_' +dd]/float(dd))\n",
    "            \n",
    "# Amount Relative velocity\n",
    "for att in entities:\n",
    "    for d in ['0','1']:\n",
    "        for dd in ['7','14','30']:\n",
    "            final[att + '_sum_amount_' + d + '_by_' +dd] \\\n",
    "            =final[att + '_cumsum_' +d]/(final[att + '_cumsum_' +dd]/float(dd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final.copy()\n",
    "data=data[(data.Date < '2010-11-01') & (data.Date > '2010-01-14')]\n",
    "random=np.random.uniform(size=len(data))\n",
    "data['random']=random\n",
    "data = data.iloc[:, np.r_[8,9,11,12,26:(len(data.columns))]]\n",
    "data = data.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KS & FDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bads = data[data['Fraud'] == 1]\n",
    "goods = data[data['Fraud'] == 0]\n",
    "\n",
    "KSFDR = pd.DataFrame([[i for i in range(1461)],data.columns]).transpose()\n",
    "KSFDR.columns = [\"Variable number\", 'Variable']\n",
    "KSFDR['ks'] = ''\n",
    "\n",
    "i = 0\n",
    "for column in data:\n",
    "    KSFDR['ks'][i] = sps.ks_2samp(goods[column],bads[column])[0]\n",
    "    i = i+1\n",
    "    \n",
    "KSFDR['rank_ks'] = KSFDR['ks'].rank(ascending = True)\n",
    "KS_result = KSFDR.sort_values(by=['rank_ks'], ascending=False)\n",
    "\n",
    "listvar = data.columns[np.r_[0,2:(len(data.columns))]]\n",
    "\n",
    "j = 1\n",
    "topRows = int(round(len(data)*0.03))\n",
    "for column in listvar:\n",
    "    temp = data[[column,'Fraud']].copy()\n",
    "    temp0 = temp.sort_values(column,ascending=False)\n",
    "    temp1 = temp0.head(topRows)\n",
    "    temp2 = temp0.tail(topRows)\n",
    "    needed1 = temp1.loc[:,'Fraud']\n",
    "    needed2 = temp2.loc[:,'Fraud']\n",
    "    FDR1 = sum(needed1)/(len(bads))\n",
    "    FDR2 = sum(needed2)/(len(bads))\n",
    "    FDRate = np.maximum(FDR1,FDR2)\n",
    "    KSFDR.loc[j, 'FDR'] = FDRate\n",
    "    j = j + 1\n",
    "    \n",
    "KSFDR['rank_FDR'] = KSFDR['FDR'].rank(ascending = True)\n",
    "\n",
    "KSFDR['average_rank'] = (KSFDR['rank_ks'] + KSFDR['rank_FDR']) / 2\n",
    "KSFDR.sort_values(by=['average_rank'], ascending=False, inplace=True)\n",
    "\n",
    "select = KSFDR.iloc[:60,1]\n",
    "select = select.tolist()\n",
    "\n",
    "new_data = data.loc[:,select]\n",
    "Y = data.loc[:,\"Fraud\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "def fdr(classifier, x, y, cutoff = 0.03):\n",
    "    return fdr_prob(y, classifier.predict_proba(x), cutoff)\n",
    "\n",
    "def fdr_prob(y, y_prob, cutoff = 0.03):\n",
    "    if len(y_prob.shape) != 1:\n",
    "        y_prob = y_prob[:,-1:]\n",
    "    num_fraud = len(y[y == 1])\n",
    "    \n",
    "    sorted_prob = np.asarray(sorted(zip(y_prob,y), key = lambda x: x[0], reverse = True))\n",
    "    \n",
    "    cutoff_bin = sorted_prob[0:int(len(y) * cutoff), 1:]\n",
    "    \n",
    "    return len(cutoff_bin[cutoff_bin == 1]) / num_fraud\n",
    "\n",
    "#model = LogisticRegression(penalty='l2', class_weight='balanced')\n",
    "model = DecisionTreeClassifier(criterion='gini',max_depth=10)\n",
    "rfecv = RFECV(estimator=model, step=1, cv=5, verbose=2, n_jobs=1, scoring=fdr)\n",
    "rfecv.fit(new_data, Y)\n",
    "\n",
    "var_selected = pd.DataFrame(sorted(zip(map(lambda x: round(x), rfecv.ranking_), new_data.columns)),\n",
    "                            columns = ['ranking', 'variable'])\n",
    "print(var_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "model = DecisionTreeClassifier(criterion='gini',max_depth=10)\n",
    "#score = make_scorer(fdr)\n",
    "sfs = SFS(model, k_features=60, forward=True, verbose=2, scoring = fdr, cv = 5)\n",
    "sfs.fit(new_data, Y)\n",
    "\n",
    "sfs.k_feature_names_\n",
    "var_data = pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1 = plot_sfs(sfs.get_metric_dict(),\n",
    "               kind = 'std_dev',\n",
    "               figsize = (20,8))\n",
    "\n",
    "#plt.ylim()\n",
    "#plt.xlim()\n",
    "plt.title('Sequential Forward Selection (w.StdDev)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
